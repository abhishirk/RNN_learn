{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "# import cPickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# For special characters\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "\n",
    "# print(sys.getdefaultencoding())\n",
    "\n",
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(BR\"\\d\")\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\" Split sentence into list of tokens \"\"\"\n",
    "    words = []\n",
    "    for space_separated_item in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_item))\n",
    "    return [w for w in words if w] # if w removes the \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'My', b'name', b'is', b'Abhishek']\n"
     ]
    }
   ],
   "source": [
    "print(basic_tokenizer(b\"My name is Abhishek\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(tokenized, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Get vocab_list, vocab_dict and rev_vocab_dict given the\n",
    "    tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Replace word count\n",
    "    vocab = {}\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "\n",
    "    # Get vocab dict (word -> token) and rev dict (token -> word)\n",
    "    vocab_dict = dict([(x,y) for (y,x) in enumerate(vocab_list)])\n",
    "    rev_vocab_dict = {v: k for k, v in vocab_dict.iteritems()}\n",
    "\n",
    "    return vocab_list, vocab_dict, rev_vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert a single sentence of words to token ids. If it is the target\n",
    "    language, we will append an EOS token to the end.\n",
    "    \"\"\"\n",
    "    if not normalize_digits:\n",
    "        # replace words not in vocab_dict with UNK_ID\n",
    "        tokens = [vocab_dict.get(w, UNK_ID) for w in sentence]\n",
    "    else:\n",
    "        tokens = [vocab_dict.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID)\n",
    "            for w in sentence]\n",
    "\n",
    "    # Append EOS token if target langauge sentence\n",
    "    if target_lang:\n",
    "        tokens.append(EOS_ID)\n",
    "    return tokens\n",
    "\n",
    "def data_to_token_ids(tokenized, vocab_dict, target_lang,\n",
    "    normalize_digits=True):\n",
    "    \"\"\"\n",
    "    Convert tokens into ids used vocab_dict and normalize all digits\n",
    "    to 0.\n",
    "    \"\"\"\n",
    "    data_as_tokens = []\n",
    "    seq_lens = []\n",
    "    max_len = max(len(sentence) for sentence in tokenized) + 1 # +1 for EOS\n",
    "\n",
    "    for sentence in tokenized:\n",
    "        token_ids = sentence_to_token_ids(sentence, vocab_dict, target_lang,\n",
    "            normalize_digits)\n",
    "        # Padding\n",
    "        data_as_tokens.append(token_ids + [PAD_ID]*(max_len - len(token_ids)))\n",
    "        # Store original sequence length\n",
    "        seq_lens.append(len(token_ids))\n",
    "\n",
    "    return np.array(data_as_tokens), np.array(seq_lens)\n",
    "\n",
    "def process_data(datafile, max_vocab_size, target_lang):\n",
    "    \"\"\"\n",
    "    Read the sentences from our datafiles.\n",
    "    \"\"\"\n",
    "    with open(datafile, 'rb') as f:\n",
    "        sentences = cPickle.load(f)\n",
    "\n",
    "    # Split into tokens\n",
    "    tokenized = []\n",
    "    for i in xrange(len(sentences)):\n",
    "        tokenized.append(basic_tokenizer(sentences[i]))\n",
    "\n",
    "    # Get vocab information\n",
    "    vocab_list, vocab_dict, rev_vocab_dict = get_vocab(tokenized,\n",
    "        max_vocab_size)\n",
    "\n",
    "    # Convert data to token ids\n",
    "    data_as_tokens, seq_lens = data_to_token_ids(tokenized, vocab_dict,\n",
    "        target_lang, normalize_digits=True)\n",
    "\n",
    "    return data_as_tokens, seq_lens, vocab_dict, rev_vocab_dict\n",
    "\n",
    "def split_data(en_token_ids, sp_token_ids,\n",
    "    en_seq_lens, sp_seq_len, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split the into train and validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    # Add go token to decoder inputs and create targets\n",
    "    for sentence in sp_token_ids:\n",
    "        decoder_inputs.append(np.array([GO_ID] + list(sentence)))\n",
    "        targets.append(np.array(([GO_ID] + list(sentence))[1:] + [0]))\n",
    "\n",
    "    sp_token_ids = np.array(decoder_inputs)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Splitting index\n",
    "    last_train_index = int(0.8*len(en_token_ids))\n",
    "\n",
    "    train_encoder_inputs = en_token_ids[:last_train_index]\n",
    "    train_decoder_inputs = sp_token_ids[:last_train_index]\n",
    "    train_targets = targets[:last_train_index]\n",
    "    train_en_seq_lens = en_seq_lens[:last_train_index]\n",
    "    train_sp_seq_len = sp_seq_len[:last_train_index]\n",
    "\n",
    "    valid_encoder_inputs = en_token_ids[last_train_index:]\n",
    "    valid_decoder_inputs = sp_token_ids[last_train_index:]\n",
    "    valid_targets = targets[last_train_index:]\n",
    "    valid_en_seq_lens = en_seq_lens[last_train_index:]\n",
    "    valid_sp_seq_len = sp_seq_len[last_train_index:]\n",
    "\n",
    "    print \"%i training samples and %i validations samples\" % (\n",
    "        len(train_encoder_inputs), len(valid_encoder_inputs))\n",
    "\n",
    "    return train_encoder_inputs, train_decoder_inputs, train_targets, \\\n",
    "        train_en_seq_lens, train_sp_seq_len, \\\n",
    "        valid_encoder_inputs, valid_decoder_inputs, valid_targets, \\\n",
    "        valid_en_seq_lens, valid_sp_seq_len\n",
    "\n",
    "\n",
    "def generate_epoch(encoder_inputs, decoder_inputs, targets, en_seq_lens, sp_seq_lens,\n",
    "    num_epochs, batch_size):\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "            en_seq_lens, sp_seq_lens, batch_size)\n",
    "\n",
    "def generate_batch(encoder_inputs, decoder_inputs, targets,\n",
    "    en_seq_lens, sp_seq_lens, batch_size):\n",
    "\n",
    "    data_size = len(encoder_inputs)\n",
    "\n",
    "    num_batches = (data_size // batch_size)\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "\n",
    "        yield encoder_inputs[start_index:end_index], \\\n",
    "            decoder_inputs[start_index:end_index], \\\n",
    "            targets[start_index:end_index], \\\n",
    "            en_seq_lens[start_index:end_index], \\\n",
    "            sp_seq_lens[start_index:end_index]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
